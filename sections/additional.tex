\section{RLHF}
\begin{enumerate}[leftmargin=*]
      \item Collect a dataset of instructions+answers and train a supervised baseline model.
      \item Produce a dataset of comparisons of different answers given by the baseline model, score them manually and train a reward model.
      \item Use PPO to fine-tune a LM (the policy) using the reward model.
\end{enumerate}

\section{Distributed SGD}
\subsection*{Communication Patterns}
Centralized, Asynchronous, Decentralized

\subsection*{Logical Channels}
Lossless, Sparsification, Quantization

\subsection*{Convergence}
C: $O(\frac{1}{\sqrt{nT}})$\\
CQ: $O(\frac{1}{\sqrt{nT}}+\frac{\epsilon}{\sqrt{T}})$\\
A: $O(\frac{1}{\sqrt{nT}}+\frac{\tau}{\sqrt{T}})$\\
D: $O(\frac{1}{\sqrt{nT}}+\frac{\rho}{{T}^{1.5}})$

\section{Data Quality}
\subsection*{Importance Measures}
LOO, Uniform Weights, Shapley Value

\section{Security}
\subsection*{Adversarial Examples}
Perturb example with noise $\delta$ to force misclassification:
\begin{equation*}
      \begin{aligned}
            \text{maximize}   & \quad L(f_\theta(x+\delta),y)  \\
            \text{subject to} & \quad \|\delta\|_\infty\leq1\%
      \end{aligned}
\end{equation*}
Solve with projected Gradient Descent. Doesn't work for text as $x+\delta$ is highly unlikely to be a token embedding. We can instead solve $\arg\max_v (E_v - x_i )^\top \nabla_{x_i}L$ and replace $x_i$ by $v$.

\section{Privacy}
\subsection*{Data Secrecy}
Central server sees all training data.\\
Gold Standard Solutions: Secure multiparty computation, fully homomorphic encryption $\leadsto$ slow \& expensive.\\
\textbf{Federated Learning}. Clients send gradients. Can be attacked with optimization. Weight-trap attack: Server sends model s.t. $\nabla_\theta L(f_\theta(x_i))=x_i.$\\

\subsection*{Data Memorization}
Can generate lots of text and filter text where model is abnormally confident.\\
Defenses:\\
1. Filter memorized outputs. Problem: Exact matches are not enough.\\
2. Deduplicate training data.

\subsection*{Differential Privacy}
An algorithm $M$ is $\varepsilon$-differentially private if for any “neighboring” databases $D_1,D_2$ that differ in a single element, and any output $S$ we have:
$$P\big[M(D_1)\in S\big] \leq \exp(\varepsilon) P\big[M(D_2)\in S\big].$$
Post-Processing: If $M$ is $\varepsilon$-DP, then $f(M)$ for any function $f$ is also $\varepsilon$-DP.\\
Composition: If $M_1$ is $\varepsilon_1$-DP and $M_2$ is $\varepsilon_2$-DP then $f(M_1,M_2)$ is $(\varepsilon_1+\varepsilon_2)$-DP.